---
title: Text Embeddings
author: Maria Lebedeva
year: 2023
tags: [nlp, embeddings, machine learning]
---

# Text Embeddings

Embeddings are dense vector representations of words, sentences, or documents that preserve semantic proximity. Models like Word2Vec, GloVe, or modern transformers (BERT, sentence‑transformers) map text to high‑dimensional vectors so that semantically close phrases are near each other in the vector space. This enables semantic search – finding content by meaning rather than exact keywords. For instance, “cat catches mouse” and “kitten hunts rodent” yield similar embeddings.
